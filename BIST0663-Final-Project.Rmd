---
title: "BISST0663_Final_Project"
author: "Jueshen Hou  Zimeng Ren"
date: "2024-10-09"
output: 
  pdf_document:
  toc: true
  number_sections: true
---


\newpage 

```{r,eval=TRUE}
##This chunk is only needed when running on Jason's laptop.If it is on Jason's device change eval=TRUE
options("install.lock"=FALSE)
```


```{r}
library(xfun)
```


```{r install package in case you do not have it, eval=FALSE, include=FALSE}
##These packages are needed for later code chunks,if you do not have the following packages installed, please run this chunk to ensure all packages needed are installed.
install.packages(c("ggplot2","broom","gridExtra","class","tidyverse","leaps","corrplot","RColorBrewer","glmnet","xtable","randomForest","pROC","devtools","UpSetR","naniar","report","rpart","rattle","rpart.plot"))
install.packages("randomForest")
install.packages("caret")
install.packages("yardstick")
```

```{r}
#This is a test2222
```


```{r load packages, include=FALSE}
##Here we load the packages needed and install a package that is not on CRAN
library(ggplot2)
library(broom)
library(gridExtra)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
library(randomForest)
library(tree)
library(pROC)
#library(smotefamily)
library(devtools)
library(UpSetR)
library(naniar)
library(report)
library(rpart)
library(rattle)
library(rpart.plot)
library(DataExplorer)
library(BART)
library(gbm)
library(caret)
```

### Load Datas
```{r}
ALZH<-read.csv("https://raw.githubusercontent.com/jasonh0509/StatsLearningFinal/refs/heads/main/alzheimers_disease_data.csv")
```

### Take a Look

```{r}
glimpse(ALZH)
```

```{r}
ALZH_noID<-ALZH[,-1]
```


```{r}
na_plot_ALZH<-vis_miss(ALZH_noID);na_plot_ALZH

```

```{r}
colSums(is.na(ALZH_noID))
```

```{r changing data types}
ALZH_noID$Diagnosis<-as.factor(ALZH_noID$Diagnosis)
ALZH_noID <- ALZH_noID %>%
  mutate(across(c(Gender, Ethnicity,EducationLevel,Smoking,FamilyHistoryAlzheimers,CardiovascularDisease,Diabetes,Depression,HeadInjury,Hypertension,MemoryComplaints,BehavioralProblems,Confusion,Disorientation,PersonalityChanges,DifficultyCompletingTasks,Forgetfulness), as.factor))
```


### Set up Data Set(Keep Same Across All Stats Leraning Models)

```{r}
ALZH.raw <- ALZH_noID %>% select(-DoctorInCharge) %>% mutate(Diagnosis = as.numeric(as.character(Diagnosis)))
ALZH.gbm <- ALZH.raw
##Dispite the name, this ALZH.gbm is the data set will be used for all models, the set in other models will be coming from this one. In other words, the copies of this set will be the "root set" of each model
```

```{r}
ALZH_for_explore<-ALZH.gbm
```

```{r}
alzh_classes<-ggplot(data = ALZH_noID, mapping = aes(x=Diagnosis,fill=Diagnosis))+
  geom_bar()+
  xlab("Diagnosis Status")+
  ggtitle("Figure x.x Classes of Alzheimer's Disease")
  
alzh_classes
```


```{r}
ggplot(data = ALZH_noID, mapping = aes(x=Diagnosis,fill=Diagnosis))+
  geom_bar()+
  xlab("Diagnosis Status")+
  ggtitle("Classes of Alzheimer's Disease After SMOTE")
  
```


```{r}
plot_intro(ALZH_noID)
```




### More EDA focused on positive cases


```{r positive only set}
alzh_pos<-subset(ALZH_noID,Diagnosis==1)

alzh_gender<-alzh_pos%>%
  group_by(Gender)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Gender, y = n,fill=Gender))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_gender+ggtitle("Alzheimer's  Across Gender")

alzh_diab<-alzh_pos%>%
  group_by(Diabetes)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Diabetes, y = n,fill=Diabetes))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_diab+ggtitle("Alzheimer's in Diabetics")
alzh_diab
```

```{r}
alzh_smoke<-alzh_pos%>%
  group_by(Smoking)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Smoking, y = n,fill=Smoking))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_smoke+ggtitle("Figure x.x Alzheimer's in Cigarette Users")

```

```{r}
alzh_edu<-alzh_pos%>%
  group_by(EducationLevel)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = EducationLevel, y = n,fill=EducationLevel))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_edu+ggtitle("Alzheimer's Disease and Education")

```

```{r}
alzh_ethnicity<-alzh_pos%>%
  group_by(Ethnicity)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Ethnicity, y = n,fill=Ethnicity))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_edu+ggtitle("Alzheimer's and Ethnicity")

```


```{r}
alzh_depression<-alzh_pos%>%
  group_by(Depression)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Depression, y = n,fill=Depression))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_depression+ggtitle("Alzheimer's and Depression")

```

```{r}
alzh_family<-alzh_pos%>%
  group_by(FamilyHistoryAlzheimers)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = FamilyHistoryAlzheimers, y = n,fill=FamilyHistoryAlzheimers))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_family+ggtitle("Alzheimer's and Family History")

```

```{r}
MMSE.scatter<-ggplot(data = ALZH_noID,mapping = aes(x=Age,y=MMSE,color=Diagnosis))+
  geom_jitter()
MMSE.scatter+ggtitle("Age vs MMSE Respect to Disease Status")
```





### Exploration of Corerlation of 4 Variables related to 

```{r}
correlation.mtx.4var<-cor(ALZH_for_explore[,c("CholesterolHDL", "CholesterolLDL", "CholesterolTotal", "CholesterolTriglycerides")], method = "kendall")
correlation.mtx.4var
correlation.mtx.4var<-as.data.frame(correlation.mtx.4var)
knitr::kable(correlation.mtx.4var)
```


```{r}
##4 logistic regression models with only response and one of the 4 cholesterol variables
hdl.logistic<-glm(Diagnosis~CholesterolHDL,data=ALZH_for_explore,family = binomial);summary(hdl.logistic)
ldl.logistic<-glm(Diagnosis~CholesterolLDL,data=ALZH_for_explore,family = binomial);summary(ldl.logistic)
total.logistic<-glm(Diagnosis~CholesterolTotal,data=ALZH_for_explore,family = binomial);summary(total.logistic)
tryglycerides.logistic<-glm(Diagnosis~CholesterolTriglycerides,data=ALZH_for_explore,family = binomial);summary(tryglycerides.logistic)

##HDL is most correlated with the response variable
```


### logistic


```{r}
ALZH.logistic<-ALZH.gbm%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))###set for logistic regression directly derived from ALZH.gbm but removed 3 variables.
n <-nrow(ALZH.logistic);n

set.seed(114514)
draw<-sample(1:n,size = 1934)##1934 is 90% of the data,here is the rows we use fall all trainig and validation sets in the project, all derived from ALZH.gbm
##This is the ultimate sample data indces!
train <-ALZH.logistic[draw,]
train_x<-train%>%dplyr::select(-Diagnosis)
train_y<-train%>%dplyr::select(Diagnosis)

test <- ALZH.logistic[-draw,]
test_x<-test%>%dplyr::select(-Diagnosis)
test_y <-test$Diagnosis


x <-model.matrix(Diagnosis~.,data=ALZH.logistic)
y <- ALZH.logistic$Diagnosis
```



```{r}
ALZH_logistic <-glm(Diagnosis~.,data=train,family = binomial)
summary(ALZH_logistic)

pred_test<-predict(ALZH_logistic,type='response',newdata = test)
glm.pred <- ifelse(pred_test > 0.5, 1, 0)
table(glm.pred, test_y)
Recall.regular.glm<-sum(glm.pred == 1 & test_y == 1)/sum(test_y == 1);Recall.regular.glm
Precision.regular.glm<-sum(glm.pred == 1 & test_y == 1)/sum(glm.pred == 1);Precision.regular.glm
F1Score.regular.glm<-2*Precision.regular.glm*Recall.regular.glm/(Precision.regular.glm+Recall.regular.glm);F1Score.regular.glm 
Accuracy.regular.glm<-sum(glm.pred == test_y)/length(test_y);Accuracy.regular.glm

Metric.regular<-data.frame("Recall"=Recall.regular.glm,"Precision"=Precision.regular.glm,"Accuracy"=Accuracy.regular.glm,"F1 score"= F1Score.regular.glm)
```


### MLR with best subset

```{r}
ALZH_leanning<-ALZH_noID%>%dplyr::select(-DoctorInCharge)
bestsubset <- regsubsets(Diagnosis~., data = ALZH_leanning)
bestsubsum<-summary(bestsubset)
bestsubsum
which.min(bestsubsum$cp)
which.min(bestsubsum$bic)
which.min(bestsubsum$adjr2)
```

```{r}
knitr::kable(coef(bestsubset,8))
bestSubset_vars <- names(coef(bestsubset, 8))[-1]
bestSubset_vars
bestSubset_STR<-paste(bestSubset_vars,collapse = ",")
```

### A MLR with best subset

```{r}
set.with.BestsubsetVar<-train%>%dplyr::select(Diagnosis,Age,SleepQuality,CholesterolHDL,MMSE,FunctionalAssessment,MemoryComplaints,BehavioralProblems,ADL)
MLR_bestsubset<-glm(Diagnosis~.,data=set.with.BestsubsetVar,family="binomial")
pred.bestsubset<-predict(MLR_bestsubset,newdata = test,type = "response",family="binomial")
class.bestsubset<-ifelse(pred.bestsubset>0.5,1,0)
table(class.bestsubset,test_y)

Recall.mlr.bestSub<-sum(class.bestsubset==1&test_y==1)/sum(test_y==1);Recall.mlr.bestSub
Precision.mlr.bestSub<-sum(class.bestsubset == 1 & test_y == 1)/sum(class.bestsubset == 1);Precision.mlr.bestSub
F1Score.mlr.bestSub<-2*Precision.mlr.bestSub*Recall.mlr.bestSub/(Precision.mlr.bestSub+Recall.mlr.bestSub);F1Score.mlr.bestSub
Accuracy.mlr.bestSub<-sum(class.bestsubset == test_y)/length(test_y);Accuracy.mlr.bestSub

Metric.mlr.bestSub<-data.frame("Recall"= Recall.mlr.bestSub,"Precision"=Precision.mlr.bestSub,"F1 score"=F1Score.mlr.bestSub,"Accuracy"=Accuracy.mlr.bestSub)
```


### lasso

```{r}
##subject to fix
library(glmnet)
grid <- 10^seq(10,-2, length = 100)
train_y.lasso<-train_y%>%mutate(Diagnosis=as.factor(Diagnosis))
train_x_lasso<-as.matrix(train_x)
lasso.mod<-glmnet(train_x_lasso,train_y.lasso$Diagnosis,alpha = 1,lambda = grid,family = "binomial")

summary(lasso.mod)
cv.out <-cv.glmnet(train_x_lasso, train_y.lasso$Diagnosis, alpha = 1,family="binomial",nfolds = 10)
plot(cv.out)
best_lambda <- cv.out$lambda.min;best_lambda
```


```{r}
train_y<-as.matrix(train_y)
lasso.final<-glmnet(train_x_lasso,train_y.lasso$Diagnosis,alpha = 1,lambda = best_lambda,family = "binomial")
lasso.pred <- predict(lasso.final, s = best_lambda, newx = as.matrix(test_x))
lasso.pred.class<-ifelse(lasso.pred > 0.5,1,0)
table(prediction=lasso.pred.class,actual=test_y)
```

```{r}
Recall.lasso<-sum(lasso.pred.class == 1 & test_y == 1)/sum(test_y == 1);Recall.lasso
Precision.lasso<-sum(lasso.pred.class == 1 & test_y == 1)/sum(lasso.pred.class == 1);Precision.lasso
F1Score.lasso<-2*Precision.lasso*Recall.lasso/(Precision.lasso+Recall.lasso);F1Score.lasso
Accuracy.lasso<-sum(lasso.pred.class == test_y)/length(test_y);Accuracy.lasso
Metric.lasso<-data.frame("Recall"=Recall.lasso,"Precision"=Precision.lasso,"Accuracy"=Accuracy.lasso,"F1 score"=F1Score.lasso)
```


```{r}
ALZH_noID_noCholest<-
  ALZH_noID%>%
  dplyr::select(-CholesterolTotal,-CholesterolHDL,-CholesterolLDL,-CholesterolTriglycerides)
```




### KNN

```{r}
ALZH_IntOnly<-ALZH_noID[,sapply(ALZH.gbm,is.integer)]
ALZH_double<-ALZH_noID[,sapply(ALZH.gbm,is.double)]
ALZH_NumOnly<-cbind(ALZH_IntOnly,ALZH_double)
ALZH_fct<-ALZH_noID[,sapply(ALZH.gbm,is.factor)]

```


```{r}
plot_correlation(ALZH_NumOnly)
```

#### Feature Selection for kNN

```{r}
#RFE.featureSet<-ALZH_NumOnly[draw,]%>%dplyr::select(-Diagnosis,DoctorInCharge)
#RFE.featureSet<-as.data.frame(RFE.featureSet)
#RFE.response<-ALZH_NumOnly[draw,]%>%dplyr::select(Diagnosis)

RFE.featureSet <- ALZH_NumOnly[draw,-which(names(ALZH_NumOnly) == "Diagnosis")]
RFE.featureSet<-RFE.featureSet[,-which(names(RFE.featureSet) == "DoctorInCharge")]
RFE.featureSet<-RFE.featureSet%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
RFE.response <- ALZH_NumOnly[draw, "Diagnosis"]

set.seed(12345)
control<-rfeControl(functions = rfFuncs, method = "cv", number = 10)
RFE.result<-rfe(RFE.featureSet,RFE.response, sizes = c(1:15), rfeControl = control)
print(RFE.result)
```

```{r}
#alzh_secondKNN and the later alzh.gbm.forTuning are the same
alzh_secondKNN<-ALZH_NumOnly[draw,]%>%
dplyr::select(FunctionalAssessment, MMSE, ADL, DietQuality, SleepQuality,Diagnosis)%>%
mutate(Diagnosis=as.factor(Diagnosis))
alzh_secondKNN.test<-ALZH.gbm[-draw,]%>%dplyr::select(FunctionalAssessment, MMSE, ADL, DietQuality, SleepQuality,Diagnosis)%>%
mutate(Diagnosis=as.factor(Diagnosis))
```


```{r}
set.seed(12345)
k_list<-seq(1,20,by=1)
nk<-length(k_list);nk
Perf.Metric.knn<-data.frame(k=rep(0,nk),Recall=rep(0,length(k_list)),Precision=rep(0,length(k_list)),F1_Score=rep(0,length(k_list)),
Accuracy=rep(0,length(k_list)))

set.seed(12345)
n<-nrow(alzh_secondKNN)
pool<-rep(1:10,ceiling(n/10))
fold<-sample(pool,n,replace = FALSE)

for(k in 1:nk){
  Perf.Metric.knn$k[k]<-k
  
  recall.sum<-0
  precision.sum<-0
  f1_score.sum<-0
  accuracy.sum<-0


  for(i in 1:10){
    #Find data in each fold
    infold<-which(fold == i)
    
    #Create training and testing sets
    Train<-alzh_secondKNN[-infold,]
    Test<-alzh_secondKNN[infold,]
    #Run kNN
    k_preds<-knn(Train%>%select(-Diagnosis),Test%>%select(-Diagnosis),k=k,cl=Train$Diagnosis)
  
    Recall<-sum(k_preds == 1 & Test$Diagnosis == 1)/sum(Test$Diagnosis == 1);recall.sum<-recall.sum+Recall
    Precision<-sum(k_preds == 1 & Test$Diagnosis == 1)/sum(k_preds == 1);precision.sum<-precision.sum+Precision
    F1_Score<-2*Precision*Recall/(Precision+Recall);f1_score.sum<-f1_score.sum+F1_Score
    Accuracy<-sum(k_preds == Test$Diagnosis)/length(Test$Diagnosis);accuracy.sum<-accuracy.sum+Accuracy

  }

    Perf.Metric.knn$Recall[k]<-recall.sum/10
    Perf.Metric.knn$Precision[k]<-precision.sum/10
    Perf.Metric.knn$F1_Score[k]<-f1_score.sum/10
    Perf.Metric.knn$Accuracy[k]<-accuracy.sum/10
    
}

Perf.Metric.knn$k[which.max(Perf.Metric.knn$Recall)]
Perf.Metric.knn
```

```{r}
knn.final<-knn(train = alzh_secondKNN%>%select(-Diagnosis),test = alzh_secondKNN.test%>%select(-Diagnosis),cl = alzh_secondKNN$Diagnosis,k=1)
table(knn.final,alzh_secondKNN.test$Diagnosis)
Recall.knn.final<-sum(knn.final == 1 & alzh_secondKNN.test$Diagnosis == 1)/sum(alzh_secondKNN.test$Diagnosis == 1);Recall.knn.final
Precision.knn.final<-sum(knn.final == 1 & alzh_secondKNN.test$Diagnosis == 1)/sum(knn.final == 1);Precision.knn.final
F1Score.knn.final<-2*Precision.knn.final*Recall.knn.final/(Precision.knn.final+Recall.knn.final);F1Score.knn.final
Accuracy.knn.final<-sum(knn.final == alzh_secondKNN.test$Diagnosis)/length(alzh_secondKNN.test$Diagnosis);Accuracy.knn.final
```
```{r}
Metric.knn<-data.frame("Recall"= Recall.knn.final,"Precision"=Precision.knn.final,"Accuracy"=Accuracy.knn.final,"F1 score"=F1Score.knn.final)
```


### gbm

```{r}
set.seed(12345)
ALZH.boosting<-ALZH.gbm%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
boosting.try <- gbm(Diagnosis ~ ., data = ALZH.boosting[draw,], distribution = "bernoulli", n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)

```

```{r}
yhat.gbm<-predict(boosting.try,newdata = ALZH.gbm[-draw,],n.trees = 5000,interaction.depth = 4,shrinkage = 0.01,type = "response")
pred_gbm_class <- ifelse(yhat.gbm > 0.5, 1, 0)
knitr::kable(table("Prediction"=pred_gbm_class,"Actual"=ALZH.gbm[-draw,]$Diagnosis))
```


```{r}
set.seed(12345)
lambda_val <- seq(0.01, 0.05, by = 0.01)
result_container <- data.frame(Lambda = lambda_val, Recall = rep(0, length(lambda_val)), Precision = rep(0, length(lambda_val)), F1_Score = rep(0, length(lambda_val)),Accuracy=rep(0,length(lambda_val)))
ALZH.boosting.forTunine<-ALZH.gbm[draw,]
ALZH.boosting.realTest<-ALZH.gbm[-draw,]

```

### Tune Together with 10 fold cv
###### This one is correct!!

```{r}
ALZH.gbm.forTuning<-ALZH.gbm[draw,]
ALZH.gbm.realTest<-ALZH.gbm[-draw,]
```



```{r,eval=TRUE,include=TRUE}
set.seed(12345)
lambda_val <- seq(0.01, 0.03, by = 0.01)
ntree_val <- c(1000, 2000, 3000)

ALZH.gbm.forGrid<-ALZH.gbm.forTuning%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
ALZH.gbm.forGrid$Diagnosis <- factor(ALZH.gbm.forGrid$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))##This ensure format align with training requirement
ALZH.gbm.realTest<-ALZH.gbm[-draw,]%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))


### Grid Creation
train.control<-trainControl(method="cv",number=10,summaryFunction=twoClassSummary,classProbs=TRUE,savePredictions=TRUE)
grid<-expand.grid(shrinkage=lambda_val,
n.trees=ntree_val,
interaction.depth=4,n.minobsinnode=10)##default is 10


set.seed(12345)
Boosting_alzh_grid <- train(
  Diagnosis ~ ., 
  data = ALZH.gbm.forGrid, 
  method = "gbm", 
  trControl = train.control, 
  tuneGrid = grid, 
  distribution = "bernoulli",
  metric = "ROC",
  verbose=TRUE,
  train.fraction = 0.9
)


gridTrainResult<-as.data.frame(Boosting_alzh_grid$results)
gridTrainResult[which.max(gridTrainResult$Sens),]
Boosting_alzh_grid$bestTune

set.seed(12345)
for.final.gbm<-ALZH.gbm.forTuning%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
Boosting_alzh_grid.final<-gbm(Diagnosis~.,data=for.final.gbm,distribution="bernoulli",n.trees=1000,interaction.depth=4,shrinkage=0.01)
yhat.boost.final<-predict(Boosting_alzh_grid.final,newdata = ALZH.gbm.realTest,n.trees = 1000,interaction.depth = 4,shrinkage = 0.01,type = "response")
pred_gbm_class_final <- ifelse(yhat.boost.final > 0.5, 1, 0)
knitr::kable(table(pred_gbm_class_final,ALZH.gbm.realTest$Diagnosis))

Recall.grid.gbm<-sum(pred_gbm_class_final == 1 & ALZH.gbm.realTest$Diagnosis == 1)/sum(ALZH.gbm.realTest$Diagnosis == 1)
Precision.grid.gbm<-sum(pred_gbm_class_final == 1 & ALZH.gbm.realTest$Diagnosis == 1)/sum(pred_gbm_class_final == 1)
F1_Score.grid.gbm<-2*Precision.grid.gbm*Recall.grid.gbm/(Precision.grid.gbm+Recall.grid.gbm)
Accuracy.grid.gbm<-sum(pred_gbm_class_final == ALZH.gbm.realTest$Diagnosis)/length(ALZH.gbm.realTest$Diagnosis)

Recall.grid.gbm
Accuracy.grid.gbm
Precision.grid.gbm
F1_Score.grid.gbm
Metric.gbm<-data.frame("Recall"= Recall.grid.gbm,"Precision"=Precision.grid.gbm,"Accuracy"=Accuracy.grid.gbm,"F1 score"=F1_Score.grid.gbm)
```









```{r chunk of gbm without parallel}

alzh.forCI<-ALZH.gbm
#alzh.forCI$Diagnosis<-as.factor(alzh.forCI$Diagnosis)
set.seed(99325)
recall.forCI<-numeric()

for (m in 1:10){
  print(m)
  cv.folds<-createFolds(alzh.forCI$Diagnosis,5)
  recall.contain<-numeric()

  for (fold in cv.folds){
    train_data<-alzh.forCI[-fold,]
    test_data<-alzh.forCI[fold,]
    
   model<-gbm(Diagnosis~.,data=train_data,distribution="bernoulli",n.trees=1000,interaction.depth=4,shrinkage=0.01)
    
    preds<-predict(model,newdata=test_data,n.trees = 1000,interaction.depth = 4,shrinkage = 0.01,type="response")
    preds_class<-ifelse(preds>0.5,1,0)
    
    
    preds_class<-as.factor(preds_class)
    test_data$Diagnosis<-as.factor(test_data$Diagnosis)
    confusion<-confusionMatrix(preds_class,test_data$Diagnosis)
    
    recall.contain<-c(recall.contain,confusion$byClass["Sensitivity"])
    mean.recall<-mean(recall.contain)
      
  }
  mean.recall<-mean(recall.contain)
  recall.forCI<-c(recall.forCI,mean.recall)

}
```




```{r attempt to parallel gbm}
library(caret)
library(gbm)
library(foreach)
library(doParallel)

num_cores <- detectCores() - 1
registerDoParallel(cores = num_cores)

set.seed(99325)
recall.forCI <- foreach(
  m = 1:100, 
  .combine = c,  
  .packages = c("caret", "gbm") 
) %dopar% {
  set.seed(99325)
  
  cv.folds <- createFolds(alzh.forCI$Diagnosis, 5)
  
 
  recall.contain <- numeric()
  
  
  for (fold in cv.folds){
    train_data <- alzh.forCI[-fold,]
    test_data <- alzh.forCI[fold,]
    
    model <- gbm(Diagnosis ~ ., data = train_data, distribution = "bernoulli", 
      n.trees = 1000, 
      interaction.depth = 4, 
      shrinkage = 0.01
    )
    
    preds <- predict(model, newdata = test_data, n.trees = 1000, interaction.depth = 4, shrinkage = 0.01, type = "response")
    
    preds_class <- ifelse(preds > 0.5, 1, 0)
    
    preds_class <- as.factor(preds_class)
    test_data$Diagnosis <- as.factor(test_data$Diagnosis)
    
    confusion <- confusionMatrix(preds_class, test_data$Diagnosis)
    
    recall.contain <- c(recall.contain, confusion$byClass["Sensitivity"])
    mean.recall <- mean(recall.contain)
  }
  
  return(mean.recall)
}


stopImplicitCluster()
```


```{r}

Metric.All<-rbind(Metric.regular,Metric.mlr.bestSub,Metric.lasso,Metric.knn,Metric.gbm)
Method<-c("Regular MLR","MLR Best Subset","Lasso","KNN","GBM")
Metric.All<-cbind(Method,Metric.All)
knitr::kable(Metric.All)
```

