---
title: "BISST0663_Final_Project"
author: "Jueshen Hou  Zimeng Ren"
date: "2024-10-09"
output: 
  pdf_document:
  toc: true
  number_sections: true
---


\newpage 

```{r,eval=TRUE}
##This chunk is only needed when running on Jason's laptop.If it is on Jason's device change eval=TRUE
options("install.lock"=FALSE)
```

```{r}
###This is a function used to make sure cluster is terminated to avoid "invalid connection" error
unregister_dopar <- function() {
    env <- foreach:::.foreachGlobals
    rm(list=ls(name=env), pos=env)
}

```



```{r}
library(xfun)
```


```{r install package in case you do not have it, eval=FALSE, include=FALSE}
##These packages are needed for later code chunks,if you do not have the following packages installed, please run this chunk to ensure all packages needed are installed.
install.packages(c("ggplot2","broom","gridExtra","class","tidyverse","leaps","corrplot","RColorBrewer","glmnet","xtable","randomForest","pROC","devtools","UpSetR","naniar","report","rpart","rattle","rpart.plot"))
install.packages("randomForest")
install.packages("caret")
install.packages("yardstick")
```



```{r load packages, include=FALSE}
##Here we load the packages needed and install a package that is not on CRAN
library(ggplot2)
library(broom)
library(gridExtra)
library(class)
library(tidyverse)
library(gridExtra)
library(leaps)
library(corrplot)
library(RColorBrewer)
library(glmnet)
library(xtable)
library(randomForest)
library(tree)
library(pROC)
#library(smotefamily)
library(devtools)
library(UpSetR)
library(naniar)
library(report)
library(rpart)
library(rattle)
library(rpart.plot)
library(DataExplorer)
library(BART)
library(gbm)
library(caret)
library(pROC)

```

### Load Datas
```{r}
ALZH<-read.csv("https://raw.githubusercontent.com/jasonh0509/StatsLearningFinal/refs/heads/main/alzheimers_disease_data.csv")
```

### Take a Look

```{r}
glimpse(ALZH)
```

```{r}
ALZH_noID<-ALZH[,-1]
```


```{r}
na_plot_ALZH<-vis_miss(ALZH_noID);na_plot_ALZH

```

```{r}
colSums(is.na(ALZH_noID))
```

```{r changing data types}
ALZH_noID$Diagnosis<-as.factor(ALZH_noID$Diagnosis)
ALZH_noID <- ALZH_noID %>%
  mutate(across(c(Gender, Ethnicity,EducationLevel,Smoking,FamilyHistoryAlzheimers,CardiovascularDisease,Diabetes,Depression,HeadInjury,Hypertension,MemoryComplaints,BehavioralProblems,Confusion,Disorientation,PersonalityChanges,DifficultyCompletingTasks,Forgetfulness), as.factor))
```


### Set up Data Set(Keep Same Across All Stats Leraning Models)

```{r}
ALZH.raw <- ALZH_noID %>% select(-DoctorInCharge) %>% mutate(Diagnosis = as.numeric(as.character(Diagnosis)))
ALZH.gbm <- ALZH.raw
##Dispite the name, this ALZH.gbm is the data set will be used for all models, the set in other models will be coming from this one. In other words, the copies of this set will be the "root set" of each model
```

```{r}
ALZH_for_explore<-ALZH.gbm
```

```{r}
alzh_classes<-ggplot(data = ALZH_noID, mapping = aes(x=Diagnosis,fill=Diagnosis))+
  geom_bar()+
  xlab("Diagnosis Status")+
  ggtitle("Figure x.x Classes of Alzheimer's Disease")
  
alzh_classes
```


```{r}
ggplot(data = ALZH_noID, mapping = aes(x=Diagnosis,fill=Diagnosis))+
  geom_bar()+
  xlab("Diagnosis Status")+
  ggtitle("Classes of Alzheimer's Disease After SMOTE")
  
```


```{r}
plot_intro(ALZH_noID)
```




### More EDA focused on positive cases


```{r positive only set}
alzh_pos<-subset(ALZH_noID,Diagnosis==1)

alzh_gender<-alzh_pos%>%
  group_by(Gender)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Gender, y = n,fill=Gender))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_gender+ggtitle("Alzheimer's  Across Gender")

alzh_diab<-alzh_pos%>%
  group_by(Diabetes)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Diabetes, y = n,fill=Diabetes))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_diab+ggtitle("Alzheimer's in Diabetics")
alzh_diab
```

```{r}
alzh_smoke<-alzh_pos%>%
  group_by(Smoking)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Smoking, y = n,fill=Smoking))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_smoke+ggtitle("Figure x.x Alzheimer's in Cigarette Users")

```

```{r}
alzh_edu<-alzh_pos%>%
  group_by(EducationLevel)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = EducationLevel, y = n,fill=EducationLevel))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_edu+ggtitle("Alzheimer's Disease and Education")

```

```{r}
alzh_ethnicity<-alzh_pos%>%
  group_by(Ethnicity)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Ethnicity, y = n,fill=Ethnicity))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_edu+ggtitle("Alzheimer's and Ethnicity")

```


```{r}
alzh_depression<-alzh_pos%>%
  group_by(Depression)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = Depression, y = n,fill=Depression))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_depression+ggtitle("Alzheimer's and Depression")

```

```{r}
alzh_family<-alzh_pos%>%
  group_by(FamilyHistoryAlzheimers)%>%
  summarise(n = n()) %>%
  ggplot(aes(x = FamilyHistoryAlzheimers, y = n,fill=FamilyHistoryAlzheimers))+
  geom_col()+
  labs(y="Count of Alzheimer's ")
alzh_family+ggtitle("Alzheimer's and Family History")

```

```{r}
MMSE.scatter<-ggplot(data = ALZH_noID,mapping = aes(x=Age,y=MMSE,color=Diagnosis))+
  geom_jitter()
MMSE.scatter+ggtitle("Age vs MMSE Respect to Disease Status")
```





### Exploration of Corerlation of 4 Variables related to 

```{r eval=FALSE, include=FALSE}
correlation.mtx.4var<-cor(ALZH_for_explore[,c("CholesterolHDL", "CholesterolLDL", "CholesterolTotal", "CholesterolTriglycerides")], method = "kendall")
correlation.mtx.4var
correlation.mtx.4var<-as.data.frame(correlation.mtx.4var)
knitr::kable(correlation.mtx.4var)
```


```{r}
##4 logistic regression models with only response and one of the 4 cholesterol variables
hdl.logistic<-glm(Diagnosis~CholesterolHDL,data=ALZH_for_explore,family = binomial);summary(hdl.logistic)
ldl.logistic<-glm(Diagnosis~CholesterolLDL,data=ALZH_for_explore,family = binomial);summary(ldl.logistic)
total.logistic<-glm(Diagnosis~CholesterolTotal,data=ALZH_for_explore,family = binomial);summary(total.logistic)
tryglycerides.logistic<-glm(Diagnosis~CholesterolTriglycerides,data=ALZH_for_explore,family = binomial);summary(tryglycerides.logistic)

##HDL is most correlated with the response variable
```


### logistic


```{r}
ALZH.logistic<-ALZH.gbm%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))###set for logistic regression directly derived from ALZH.gbm but removed 3 variables.
ALZH.logistic$Diagnosis<-as.factor(ALZH.logistic$Diagnosis)
n <-nrow(ALZH.logistic);n

#set.seed(114514)
draw<-(1:2149)
#draw<-sample(1:n,size = 1934)##1934 is 90% of the data,here is the rows we use fall all trainig and validation sets in the project, all derived from ALZH.gbm
##This is the ultimate sample data indces!
train <-ALZH.logistic[draw,]
train_x<-train%>%dplyr::select(-Diagnosis)
train_y<-train%>%dplyr::select(Diagnosis)


x <-model.matrix(Diagnosis~.,data=ALZH.logistic)
y <- ALZH.logistic$Diagnosis
```


```{r}
set.seed(1234)

#Create metric containers
#auc.values <- c()
#recall.val<-c()
#precision.val<-c()
#accuracy.val<-c()

recall.set<-c()
precision.set<-c()
accuracy.set<-c()
auc.set<-c()


# Perform repeated cross-validation
for (r in 1:300) {
  cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(ALZH.logistic$Diagnosis, k = 5)
  #print(folds)
  recall.vect<-c()
  precision.vect<-c()
  accuracy.vect<-c()
  auc.vect<-c()

  for (i in seq_along(folds)) {
    cat(" Fold:", i, "\n")
    
    # Get index for training and test sets
    #print(folds[[i]])
    test.index <- folds[[i]]
    train.index <- -test.index

    train.fold <- ALZH.logistic[train.index, ]
    test.fold <- ALZH.logistic[test.index, ]
    
    # Fit the logistic regression model
    model <- glm(Diagnosis ~ ., data = train.fold, family = binomial)
    
    # Get predictions and calculate probabilities on the test set
    probs <- predict(model, test.fold, type = "response")
    preds<-ifelse(probs>0.5,1,0)
    preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(preds,actual,positive = "1")

    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == "1" & actual == "1")/sum(actual == "1")
    #recall2<-mtx$byClass["Sensitivity"]
    recall.vect<-c(recall.vect,recall)
    #print(recall.vect)
    #print(recall)
    #print(recall2)
    precision <- mtx$byClass["Pos Pred Value"]
    precision.vect<-c(precision.vect,precision)
    
    accuracy<-mtx$overall["Accuracy"]
    accuracy.vect<-c(accuracy.vect,accuracy)
    
    roc.curve <- roc(test.fold$Diagnosis, probs)
    auc <- auc(roc.curve)
    auc.vect<-c(auc.vect,auc)
    
    
  }
  #print(recall.vect)
  #print(mean(recall.vect))
  recall.set<-c(recall.set,mean(recall.vect))
  precision.set<-c(precision.set,mean(precision.vect))
  accuracy.set<-c(accuracy.set,mean(accuracy.vect))
  auc.set<-c(auc.set,mean(auc.vect))
  
}

```


```{r}
mean.recall.final<-round(mean(recall.set),3);mean.recall.final
CI.recall.final<-round(t.test(recall.set)$conf.int,5);CI.recall.final

mean.precision.final<-round(mean(precision.set),3);mean.precision.final
CI.precision.final<-round(t.test(precision.set)$conf.int,5);CI.precision.final

mean.accuracy.final<-round(mean(accuracy.set),3); mean.accuracy.final
CI.accuracy.final<-round(t.test(accuracy.set)$conf.int,5);CI.accuracy.final

mean.auc.final<-round(mean(auc.set),4);mean.auc.final
CI.auc.final<-round(t.test(auc.set)$conf.int,5);CI.auc.final


```





### MLR with best subset

```{r include=FALSE}
ALZH_leanning<-ALZH_noID%>%dplyr::select(-DoctorInCharge)
bestsubset <- regsubsets(Diagnosis~., data = ALZH_leanning)
bestsubsum<-summary(bestsubset)
bestsubsum
which.min(bestsubsum$cp)
which.min(bestsubsum$bic)
which.min(bestsubsum$adjr2)
```

```{r}
knitr::kable(coef(bestsubset,8))
bestSubset_vars <- names(coef(bestsubset, 8))[-1]
bestSubset_vars
bestSubset_STR<-paste(bestSubset_vars,collapse = ",")
```

### A MLR with best subset

```{r}
set.with.BestsubsetVar<-ALZH.logistic%>%dplyr::select(Diagnosis,Age,SleepQuality,CholesterolHDL,MMSE,FunctionalAssessment,MemoryComplaints,BehavioralProblems,ADL)
```



```{r}
##Repeated cv MLR bestsubset
set.seed(1234)

#Create metric containers
#auc.values <- c()
#recall.val<-c()
#precision.val<-c()
#accuracy.val<-c()

recall.set.sb<-c()
precision.set.sb<-c()
accuracy.set.sb<-c()
auc.set.sb<-c()


# Perform repeated cross-validation
for (r in 1:300) {
  cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(set.with.BestsubsetVar$Diagnosis, k = 5)
  #print(folds)
  recall.vect.sb<-c()
  precision.vect.sb<-c()
  accuracy.vect.sb<-c()
  auc.vect.sb<-c()

  for (i in seq_along(folds)) {
    
    # Get index for training and test sets
    #print(folds[[i]])
    test.index <- folds[[i]]
    train.index <- -test.index

    train.fold <- set.with.BestsubsetVar[train.index, ]
    test.fold <- set.with.BestsubsetVar[test.index, ]
    
    # Fit the logistic regression model
    model <- glm(Diagnosis ~ ., data = train.fold, family = binomial)
    
    # Get predictions and calculate probabilities on the test set
    probs <- predict(model, test.fold, type = "response")
    preds<-ifelse(probs>0.5,1,0)
    preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(preds,actual,positive = "1")

    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == "1" & actual == "1")/sum(actual == "1")
    #recall2<-mtx$byClass["Sensitivity"]
    recall.vect.sb<-c(recall.vect.sb,recall)
    #print(recall.vect)
    #print(recall)
    #print(recall2)
    precision <- mtx$byClass["Pos Pred Value"]
    precision.vect.sb<-c(precision.vect.sb,precision)
    #print(precision.vect)
    
    accuracy<-mtx$overall["Accuracy"]
    accuracy.vect.sb<-c(accuracy.vect.sb,accuracy)
    
    #test.fold$Diagnosis <- as.numeric(as.character(test.fold$Diagnosis))
    roc.curve <- roc(test.fold$Diagnosis, probs)
    auc <- auc(roc.curve)
    auc.vect.sb<-c(auc.vect.sb,auc)
    
    
  }
  #print(recall.vect)
  #print(mean(recall.vect))
  recall.set.sb<-c(recall.set.sb,mean(recall.vect.sb))
  precision.set.sb<-c(precision.set.sb,mean(precision.vect.sb))
  accuracy.set.sb<-c(accuracy.set.sb,mean(accuracy.vect.sb))
  auc.set.sb<-c(auc.set.sb,mean(auc.vect.sb))
  
}

```

```{r}
mean.recallBestSub.final<-round(mean(recall.set.sb),3);mean.recallBestSub.final
CI.recallSb.final<-round(t.test(recall.set.sb)$conf.int,5);CI.recallSb.final

mean.precisionBestSub.final<-round(mean(precision.set.sb),3);mean.precisionBestSub.final
CI.precisionSb.final<-round(t.test(precision.set.sb)$conf.int,5);CI.precisionSb.final

mean.accuracySb.final<-round(mean(accuracy.set.sb),3); mean.accuracySb.final
CI.accuracySb.final<-round(t.test(accuracy.set.sb)$conf.int,5);CI.accuracySb.final

mean.aucSb.final<-round(mean(auc.set.sb),3);mean.aucSb.final
CI.aucSb.final<-round(t.test(auc.set.sb)$conf.int,5);CI.aucSb.final
```


### lasso

```{r}
##subject to fix
library(glmnet)
grid <- 10^seq(10,-2, length = 100)
train_y.lasso<-train_y%>%mutate(Diagnosis=as.factor(Diagnosis))
train_x_lasso<-as.matrix(train_x)
lasso.mod<-glmnet(train_x_lasso,train_y.lasso$Diagnosis,alpha = 1,lambda = grid,family = "binomial")

summary(lasso.mod)
cv.out <-cv.glmnet(train_x_lasso, train_y.lasso$Diagnosis, alpha = 1,family="binomial",nfolds = 10)
plot(cv.out)
best_lambda <- cv.out$lambda.min;best_lambda
```


```{r}
train_y<-as.matrix(train_y)
lasso.final<-glmnet(train_x_lasso,train_y.lasso$Diagnosis,alpha = 1,lambda = best_lambda,family = "binomial")

```



```{r eval=FALSE, include=FALSE}
#repeated CV for Lasso

#Create metric containers
auc.values.lasso <- c()
recall.val.lasso<-c()
precision.val.lasso<-c()
accuracy.val.lasso<-c()

auc.lasso.vase<-c()
recall.lasso.vase<-c()
precision.lasso.vase<-c()
accuracy.lasso.vase<-c()

# Perform repeated cross-validation
for (r in 1:50) {
  cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(ALZH.logistic$Diagnosis, k = 10)
  
  for (i in seq_along(folds)) {
    cat(" Fold:", i, "\n")
    
    # Get index for training and test sets
    test.index <- folds[[i]]
    train.index <- -test_indices

    train.fold <- ALZH.logistic[train.index, ]
    test.fold <- ALZH.logistic[test.index, ]
    
    # Fit the logistic regression model
    model <- glm(Diagnosis ~ ., data = train.fold, family = binomial)
    
    # Get predictions and calculate probabilities on the test set
    probs <- predict(model, test.fold, type = "response")
    preds<-ifelse(probs>0.5,1,0)
    preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(preds,actual)

    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == 1 & test.fold$Diagnosis == 1)/sum(test.fold$Diagnosis == 1)
    precision <- mtx$byClass["Precision"]
    accuracy<-mtx$overall["Accuracy"]
    roc.curve <- roc(test.fold$Diagnosis, probs)
    auc <- auc(roc.curve)
    
    # Store the metric value
    auc.values.lasso <- c(auc.values.lasso, auc)
    recall.val.lasso<-c(recall.val.lasso,recall)
    precision.val.lasso<-c(precision.val.lasso,precision)
    accuracy.val.lasso<-c(accuracy.val.lasso,accuracy)
    
    mean.auc.lasso<-mean(auc.values.lasso)
    mean.recall.lasso<-mean(recall.val.lasso)
    mean.precision.lasso<-mean(precision.val.lasso)
    mean.accuracy.lasso<-mean(accuracy.val.lasso)
    
    auc.lasso.vase<-c(auc.lasso.vase,mean.auc.lasso)
    recall.lasso.vase<-c(recall.lasso.vase,mean.recall.lasso)
    precision.lasso.vase<-c(precision.lasso.vase,mean.precision.lasso)
    accuracy.lasso.vase<-c(accuracy.lasso.vase,mean.accuracy.lasso)
  }
  
}

```


```{r message=FALSE, warning=FALSE}
set.seed(1234)

#Create metric containers
#auc.values <- c()
#recall.val<-c()
#precision.val<-c()
#accuracy.val<-c()

recall.set.lasso<-c()
precision.set.lasso<-c()
accuracy.set.lasso<-c()
auc.set.lasso<-c()


# Perform repeated cross-validation
for (r in 1:300) {
  cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(ALZH.logistic$Diagnosis, k = 5)
  #print(folds)
  recall.vect.lasso<-c()
  precision.vect.lasso<-c()
  accuracy.vect.lasso<-c()
  auc.vect.lasso<-c()

  for (i in seq_along(folds)) {
    cat(" Fold:", i, "\n")
    
    # Get index for training and test sets
    #print(folds[[i]])
    test.index <- folds[[i]]
    train.index <- -test.index

    train.fold <- ALZH.logistic[train.index, ]
    test.fold <- ALZH.logistic[test.index, ]
    
    # Fit the logistic regression model
    model <- glm(Diagnosis ~ ., data = train.fold, family = binomial)
    
    # Get predictions and calculate probabilities on the test set
    probs <- predict(model, test.fold, type = "response")
    preds<-ifelse(probs>0.5,1,0)
    preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(preds,actual,positive = "1")

    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == "1" & actual == "1")/sum(actual == "1")
    #recall2<-mtx$byClass["Sensitivity"]
    recall.vect.lasso<-c(recall.vect.lasso,recall)
    #print(recall.vect)
    #print(recall)
    #print(recall2)
    precision <- mtx$byClass["Pos Pred Value"]
    precision.vect.lasso<-c(precision.vect.lasso,precision)
    
    accuracy<-mtx$overall["Accuracy"]
    accuracy.vect.lasso<-c(accuracy.vect.lasso,accuracy)
    
    roc.curve <- roc(test.fold$Diagnosis, probs)
    auc <- auc(roc.curve)
    auc.vect.lasso<-c(auc.vect.lasso,auc)
    
    
  }
  #print(recall.vect)
  #print(mean(recall.vect))
  recall.set.lasso<-c(recall.set.lasso,mean(recall.vect.lasso))
  precision.set.lasso<-c(precision.set.lasso,mean(precision.vect.lasso))
  accuracy.set.lasso<-c(accuracy.set.lasso,mean(accuracy.vect.lasso))
  auc.set.lasso<-c(auc.set.lasso,mean(auc.vect.lasso))
  
}

```

```{r}
mean.recallLasso.final<-round(mean(recall.set.lasso),4);mean.recallLasso.final
CI.recallLasso.final<-round(t.test(recall.set.lasso)$conf.int,5);CI.recallLasso.final

mean.precisionLasso.final<-round(mean(precision.set.lasso),4);mean.precisionLasso.final
CI.precisionLasso.final<-round(t.test(precision.set.lasso)$conf.int,5);CI.precisionLasso.final

mean.accuracyLasso.final<-round(mean(accuracy.set.lasso),4); mean.accuracyLasso.final
CI.accuracyLasso.final<-round(t.test(accuracy.set.lasso)$conf.int,5);CI.accuracyLasso.final

mean.aucLasso.final<-round(mean(auc.set.lasso),4);mean.aucLasso.final
CI.aucLasso.final<-round(t.test(auc.set.lasso)$conf.int,5);CI.aucLasso.final
```


```{r}
ALZH_noID_noCholest<-
  ALZH_noID%>%
  dplyr::select(-CholesterolTotal,-CholesterolHDL,-CholesterolLDL,-CholesterolTriglycerides)
```




### KNN

```{r}
ALZH_IntOnly<-ALZH_noID[,sapply(ALZH.gbm,is.integer)]
ALZH_double<-ALZH_noID[,sapply(ALZH.gbm,is.double)]
ALZH_NumOnly<-cbind(ALZH_IntOnly,ALZH_double)
ALZH_fct<-ALZH_noID[,sapply(ALZH.gbm,is.factor)]

```


```{r}
plot_correlation(ALZH_NumOnly)
```

#### Feature Selection for kNN

```{r}
#RFE.featureSet<-ALZH_NumOnly[draw,]%>%dplyr::select(-Diagnosis,DoctorInCharge)
#RFE.featureSet<-as.data.frame(RFE.featureSet)
#RFE.response<-ALZH_NumOnly[draw,]%>%dplyr::select(Diagnosis)

RFE.featureSet <- ALZH_NumOnly[draw,-which(names(ALZH_NumOnly) == "Diagnosis")]
RFE.featureSet<-RFE.featureSet[,-which(names(RFE.featureSet) == "DoctorInCharge")]
RFE.featureSet<-RFE.featureSet%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
RFE.response <- ALZH_NumOnly[draw, "Diagnosis"]

set.seed(12345)
control<-rfeControl(functions = rfFuncs, method = "cv", number = 10)
RFE.result<-rfe(RFE.featureSet,RFE.response, sizes = c(1:15), rfeControl = control)
print(RFE.result)
```

```{r}
#alzh_secondKNN and the later alzh.gbm.forTuning are the same
alzh_secondKNN<-ALZH_NumOnly[draw,]%>%
dplyr::select(FunctionalAssessment, MMSE, ADL, DietQuality, SleepQuality,Diagnosis)%>%
mutate(Diagnosis=as.factor(Diagnosis))
alzh_secondKNN.test<-ALZH.gbm[-draw,]%>%dplyr::select(FunctionalAssessment, MMSE, ADL, DietQuality, SleepQuality,Diagnosis)%>%
mutate(Diagnosis=as.factor(Diagnosis))
```


```{r}
set.seed(12345)
k_list<-seq(1,20,by=1)
nk<-length(k_list);nk
Perf.Metric.knn<-data.frame(k=rep(0,nk),Recall=rep(0,length(k_list)),Precision=rep(0,length(k_list)),F1_Score=rep(0,length(k_list)),
Accuracy=rep(0,length(k_list)))

set.seed(12345)
n<-nrow(alzh_secondKNN)
pool<-rep(1:10,ceiling(n/10))
fold<-sample(pool,n,replace = FALSE)

for(k in 1:nk){
  Perf.Metric.knn$k[k]<-k
  
  recall.sum<-0
  precision.sum<-0
  f1_score.sum<-0
  accuracy.sum<-0


  for(i in 1:10){
    #Find data in each fold
    infold<-which(fold == i)
    
    #Create training and testing sets
    Train<-alzh_secondKNN[-infold,]
    Test<-alzh_secondKNN[infold,]
    #Run kNN
    k_preds<-knn(Train%>%select(-Diagnosis),Test%>%select(-Diagnosis),k=k,cl=Train$Diagnosis)
  
    Recall<-sum(k_preds == 1 & Test$Diagnosis == 1)/sum(Test$Diagnosis == 1);recall.sum<-recall.sum+Recall
    Precision<-sum(k_preds == 1 & Test$Diagnosis == 1)/sum(k_preds == 1);precision.sum<-precision.sum+Precision
    F1_Score<-2*Precision*Recall/(Precision+Recall);f1_score.sum<-f1_score.sum+F1_Score
    Accuracy<-sum(k_preds == Test$Diagnosis)/length(Test$Diagnosis);accuracy.sum<-accuracy.sum+Accuracy

  }

    Perf.Metric.knn$Recall[k]<-recall.sum/10
    Perf.Metric.knn$Precision[k]<-precision.sum/10
    Perf.Metric.knn$F1_Score[k]<-f1_score.sum/10
    Perf.Metric.knn$Accuracy[k]<-accuracy.sum/10
    
}

Perf.Metric.knn$k[which.max(Perf.Metric.knn$Recall)]
Perf.Metric.knn
```

```{r eval=FALSE, include=FALSE}
knn.final<-knn(train = alzh_secondKNN%>%select(-Diagnosis),test = alzh_secondKNN%>%select(-Diagnosis),cl = alzh_secondKNN$Diagnosis,k=1)
table(knn.final,alzh_secondKNN$Diagnosis)
Recall.knn.final<-sum(knn.final == 1 & alzh_secondKNN.test$Diagnosis == 1)/sum(alzh_secondKNN.test$Diagnosis == 1);Recall.knn.final
Precision.knn.final<-sum(knn.final == 1 & alzh_secondKNN.test$Diagnosis == 1)/sum(knn.final == 1);Precision.knn.final
F1Score.knn.final<-2*Precision.knn.final*Recall.knn.final/(Precision.knn.final+Recall.knn.final);F1Score.knn.final
Accuracy.knn.final<-sum(knn.final == alzh_secondKNN.test$Diagnosis)/length(alzh_secondKNN.test$Diagnosis);Accuracy.knn.final
```






```{r message=FALSE, warning=FALSE}
set.seed(1234)

#Create metric containers
#auc.values <- c()
#recall.val<-c()
#precision.val<-c()
#accuracy.val<-c()

recall.set.knn<-c()
precision.set.knn<-c()
accuracy.set.knn<-c()
auc.set.knn<-c()


# Perform repeated cross-validation
for (r in 1:300) {
  #cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(alzh_secondKNN$Diagnosis, k = 5)
  #print(folds)
  recall.vect.knn<-c()
  precision.vect.knn<-c()
  accuracy.vect.knn<-c()
  auc.vect.knn<-c()

  for (i in seq_along(folds)) {
    #cat(" Fold:", i, "\n")
    
    # Get index for training and test sets
    #print(folds[[i]])
    test.index <- folds[[i]]
    train.index <- -test.index

    train.fold <- alzh_secondKNN[train.index, ]
    test.fold <- alzh_secondKNN[test.index, ]
    
    # Fit the logistic regression model
    model <- knn(train = train.fold%>%select(-Diagnosis),test = test.fold%>%select(-Diagnosis),cl = train.fold$Diagnosis,k=1)
    
    # Get predictions and calculate probabilities on the test set
    #probs <- predict(model, test.fold, type = "response")
    #preds<-ifelse(probs>0.5,1,0)
    #preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(model,actual,positive = "1")

    
    #get recall and precision and accuracy and auc value
    recall<-sum(model == "1" & actual == "1")/sum(actual == "1")
    #recall2<-mtx$byClass["Sensitivity"]
    recall.vect.knn<-c(recall.vect.knn,recall)
    #print(recall.vect)
    #print(recall)
    #print(recall2)
    precision <- mtx$byClass["Pos Pred Value"]
    precision.vect.knn<-c(precision.vect.knn,precision)
    
    accuracy<-mtx$overall["Accuracy"]
    accuracy.vect.knn<-c(accuracy.vect.knn,accuracy)
    
    roc.curve <- roc(test.fold$Diagnosis, as.numeric(model))
    auc <- auc(roc.curve)
    auc.vect.knn<-c(auc.vect.knn,auc)
    
    
  }
  #print(recall.vect)
  #print(mean(recall.vect))
  recall.set.knn<-c(recall.set.knn,mean(recall.vect.knn))
  precision.set.knn<-c(precision.set.knn,mean(precision.vect.knn))
  accuracy.set.knn<-c(accuracy.set.knn,mean(accuracy.vect.knn))
  auc.set.knn<-c(auc.set.knn,mean(auc.vect.knn))
  
}

```

```{r}
mean.recallknn.final<-round(mean(recall.set.knn),4);mean.recallknn.final
CI.recallknn.final<-round(t.test(recall.set.knn)$conf.int,5);CI.recallknn.final

mean.precisionknn.final<-round(mean(precision.set.knn),4);mean.precisionknn.final
CI.precisionknn.final<-round(t.test(precision.set.knn)$conf.int,5);CI.precisionknn.final

mean.accuracyknn.final<-round(mean(accuracy.set.knn),4); mean.accuracyknn.final
CI.accuracyknn.final<-round(t.test(accuracy.set.knn)$conf.int,5);CI.accuracyknn.final

mean.aucknn.final<-round(mean(auc.set.knn),4);mean.aucknn.final
CI.aucknn.final<-round(t.test(auc.set.knn)$conf.int,5);CI.aucknn.final

```


### gbm

```{r}
set.seed(12345)
ALZH.boosting<-ALZH.gbm%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
```




```{r}
set.seed(12345)
lambda_val <- seq(0.01, 0.05, by = 0.01)
result_container <- data.frame(Lambda = lambda_val, Recall = rep(0, length(lambda_val)), Precision = rep(0, length(lambda_val)), F1_Score = rep(0, length(lambda_val)),Accuracy=rep(0,length(lambda_val)))
ALZH.boosting.forTunine<-ALZH.gbm[draw,]
#ALZH.boosting.realTest<-ALZH.gbm[-draw,]

```

### Tune Together with 10 fold cv
###### This one is correct!!

```{r}
ALZH.gbm.forTuning<-ALZH.gbm[draw,]
#ALZH.gbm.realTest<-ALZH.gbm[-draw,]
```


```{r eval=FALSE, include=FALSE}
library(parallel)
library(doParallel)
#add an RFE
rfe.featureGBM<-ALZH.gbm.forTuning[draw,-which(names(ALZH.gbm.forTuning) == "Diagnosis")]
rfe.featureGBM<-rfe.featureGBM%>%dplyr::select(-CholesterolTotal,-CholesterolHDL,-CholesterolLDL,-CholesterolTriglycerides)
RFE.responseGBM<-ALZH.gbm.forTuning[draw,"Diagnosis"]
RFE.responseGBM<-as.factor(RFE.responseGBM)


num_cores <- detectCores() - 1  # Use one less than the total number of cores
cl2 <- makeCluster(num_cores)    # Create a cluster
registerDoParallel(cl2) 

set.seed(1124)
control.gbm<-rfeControl(functions = rfFuncs, method = "cv", number = 10,allowParallel = TRUE)
RFE.result.gbm<-rfe(rfe.featureGBM,RFE.responseGBM, sizes =c(5,10,15) , rfeControl = control.gbm)
print(RFE.result.gbm)

stopCluster(cl2)

```


```{r include=FALSE}
closeAllConnections()
unregister_dopar()
```




```{r,eval=TRUE,include=TRUE}
set.seed(12345)
lambda_val <- seq(0.01, 0.03, by = 0.01)
ntree_val <- c(1000, 2000, 3000)

ALZH.gbm.forGrid<-ALZH.gbm.forTuning%>%select(c(FunctionalAssessment, ADL, MMSE, MemoryComplaints, BehavioralProblems,CholesterolHDL,Diagnosis))
ALZH.gbm.forGrid$Diagnosis <- factor(ALZH.gbm.forGrid$Diagnosis, levels = c(0, 1), labels = c("No", "Yes"))
ALZH.gbm.realTest<-ALZH.gbm[-draw,]%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))


### Grid Creation
train.control<-trainControl(method="cv",number=10,summaryFunction=twoClassSummary,classProbs=TRUE,savePredictions=TRUE)
grid<-expand.grid(shrinkage=lambda_val,
n.trees=ntree_val,
interaction.depth=4,n.minobsinnode=10)##default is 10


set.seed(12345)
Boosting_alzh_grid <- train(
  Diagnosis ~ ., 
  data = ALZH.gbm.forGrid, 
  method = "gbm", 
  trControl = train.control, 
  tuneGrid = grid, 
  distribution = "bernoulli",
  metric = "ROC",
  verbose=FALSE,
  train.fraction = 0.9
)


gridTrainResult<-as.data.frame(Boosting_alzh_grid$results)
gridTrainResult[which.max(gridTrainResult$Sens),]
Boosting_alzh_grid$bestTune

set.seed(12345)
for.final.gbm<-ALZH.gbm.forTuning%>%select(-c(CholesterolTotal,CholesterolLDL,CholesterolTriglycerides))
Boosting_alzh_grid.final<-gbm(Diagnosis~.,data=for.final.gbm,distribution="bernoulli",n.trees=1000,interaction.depth=4,shrinkage=0.01)
yhat.boost.final<-predict(Boosting_alzh_grid.final,newdata = ALZH.gbm.realTest,n.trees = 1000,interaction.depth = 4,shrinkage = 0.01,type = "response")
pred_gbm_class_final <- ifelse(yhat.boost.final > 0.5, 1, 0)
knitr::kable(table(pred_gbm_class_final,ALZH.gbm.realTest$Diagnosis))

Recall.grid.gbm<-sum(pred_gbm_class_final == 1 & ALZH.gbm.realTest$Diagnosis == 1)/sum(ALZH.gbm.realTest$Diagnosis == 1)
Precision.grid.gbm<-sum(pred_gbm_class_final == 1 & ALZH.gbm.realTest$Diagnosis == 1)/sum(pred_gbm_class_final == 1)
F1_Score.grid.gbm<-2*Precision.grid.gbm*Recall.grid.gbm/(Precision.grid.gbm+Recall.grid.gbm)
Accuracy.grid.gbm<-sum(pred_gbm_class_final == ALZH.gbm.realTest$Diagnosis)/length(ALZH.gbm.realTest$Diagnosis)

Recall.grid.gbm
Accuracy.grid.gbm
Precision.grid.gbm
F1_Score.grid.gbm
Metric.gbm<-data.frame("Recall"= Recall.grid.gbm,"Precision"=Precision.grid.gbm,"Accuracy"=Accuracy.grid.gbm,"F1 score"=F1_Score.grid.gbm)
```









```{r chunk of gbm without parallel, eval=FALSE, include=FALSE}
library(pROC)

#alzh.forCI$Diagnosis<-as.factor(alzh.forCI$Diagnosis)
set.seed(99325)
recall.forCI<-numeric()
precision.forCI<-numeric()
accuracy.forCI<-numeric()
roc.forCI<-numeric()

mean.recallCI<-numeric()
mean.precisionCI<-numeric()
mean.accuracyCI<-numeric()
mean.rocCI<-numeric()

for (m in 1:10){
  print(m)
  cv.folds<-createFolds(alzh.forCI$Diagnosis,5)
  recall.contain<-numeric()
  precision.contain<-numeric()
  accuracy.contain<-numeric()
  #roc.contain<-numeric()

  for (fold in cv.folds){
    train_data<-alzh.forCI[-fold,]
    test_data<-alzh.forCI[fold,]
    
   model<-gbm(Diagnosis~.,data=train_data,distribution="bernoulli",n.trees=1000,interaction.depth=4,shrinkage=0.01)
    
    preds<-predict(model,newdata=test_data,n.trees = 1000,interaction.depth = 4,shrinkage = 0.01,type="response")
    preds_class<-ifelse(preds>0.5,1,0)
    
    
    preds_class<-as.factor(preds_class)
    test_data$Diagnosis<-as.factor(test_data$Diagnosis)
    confusion<-confusionMatrix(preds_class,test_data$Diagnosis)
    
    recall.contain<-c(recall.contain,confusion$byClass["Sensitivity"])
    precision.contain<-c(precision.contain,confusion$byClass["Precision"])
    accuracy.contain<-c(accuracy.contain,confusion$byClass["Accuracy"])
    rocCurve<-roc(test_data$Diagnosis,preds)
    auc.contain<-pROC::auc(rocCurve)
    
    #mean.recall<-mean(recall.contain)
      
  }
  mean.recallCI<-mean(recall.contain)
  recall.forCI<-c(recall.forCI,mean.recallCI)
  
  mean.precisionCI<-mean(precision.contain)
  precision.forCI<-c(precision.forCI,mean.precisionCI)
  
  mean.accuracyCI<-mean(accuracy.contain)
  accuracy.forCI<-c(accuracy.forCI,mean.accuracyCI)
  
  mean.rocCI<-mean(auc.contain)
  roc.forCI<-c(roc.forCI,mean.rocCI)
  
  

}
```




```{r attempt to parallel gbm, eval=FALSE, include=FALSE}
library(caret)
library(gbm)
library(foreach)
library(doParallel)
library(pROC)




# Prepare for parallelization
num_cores <- detectCores() - 1
registerDoParallel(cores = num_cores)

# Parallelize the outer loop
set.seed(99325)
recall.forCI <- foreach(
  m = 1:10, 
  .combine = c,  # Combine results into a vector
  .packages = c("caret", "gbm")  # Libraries to load in each worker
) %dopar% {
  # Create folds
  cv.folds <- createFolds(alzh.forCI$Diagnosis, 5)
  
  # Initialize recall container for this iteration
  recall.contain <- numeric()
  auc.values.gbm <- c()
  #recall.val.gbm<-c()
  precision.val.gbm<-c()
  accuracy.val.gbm<-c()

  auc.gbm.vase<-c()
  recall.gbm.vase<-c()
  precision.gbm.vase<-c()
  accuracy.gbm.vase<-c()
  
  # Inner loop (now sequential within each parallel iteration)
  for (fold in cv.folds){
    train_data <- alzh.forCI[-fold,]
    test_data <- alzh.forCI[fold,]
    
    model <- gbm(
      Diagnosis ~ ., 
      data = train_data, 
      distribution = "bernoulli", 
      n.trees = 2000, 
      interaction.depth = 4, 
      shrinkage = 0.03
    )
    
    preds <- predict(
      model, 
      newdata = test_data, 
      n.trees = 2000, 
      interaction.depth = 4, 
      shrinkage = 0.03, 
      type = "response"
    )
    
    preds_class <- ifelse(preds > 0.5, 1, 0)
    
    preds_class <- as.factor(preds_class)
    test_data$Diagnosis <- as.factor(test_data$Diagnosis)
    
    confusion <- confusionMatrix(preds_class, test_data$Diagnosis)
    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == 1 & test_data$Diagnosis == 1)/sum(test_data$Diagnosis == 1)
    precision <- confusion$byClass["Precision"]
    accuracy<-confusion$overall["Accuracy"]
    roc.curve <- pROC::roc(test_data$Diagnosis, preds)
    auc <- pROC::auc(roc.curve)
    
    # Store the metric value
    auc.values.gbm <- c(auc.values.gbm, auc)
    recall.val.gbm<-c(recall.val.gbm,recall)
    precision.val.gbm<-c(precision.val.gbm,precision)
    accuracy.val.gbm<-c(accuracy.val.gbm,accuracy)
    
    mean.auc.gbm<-mean(auc.values.gbm)
    mean.recall.gbm<-mean(recall.val.gbm)
    mean.precision.gbm<-mean(precision.val.gbm)
    mean.accuracy.gbm<-mean(accuracy.val.gbm)
    
    auc.gbm.vase<-c(auc.gbm.vase,mean.auc.gbm)
    recall.gbm.vase<-c(recall.gbm.vase,mean.recall.gbm)
    precision.gbm.vase<-c(precision.gbm.vase,mean.precision.gbm)
    accuracy.gbm.vase<-c(accuracy.gbm.vase,mean.accuracy.gbm)
  }
  
  #return(mean.recall)
}

# Stop the parallel backend
stopImplicitCluster()
unregister_dopar()
```






```{r true gbm cv, message=FALSE, warning=FALSE}
alzh.forCI<-ALZH.gbm.forGrid
alzh.forCI <- alzh.forCI %>%
  mutate(Diagnosis = ifelse(Diagnosis == "Yes", 1, 0))
set.seed(1234)

#Create metric containers
#auc.values <- c()
#recall.val<-c()
#precision.val<-c()
#accuracy.val<-c()

recall.set.gbm<-c()
precision.set.gbm<-c()
accuracy.set.gbm<-c()
auc.set.gbm<-c()


# Perform repeated cross-validation
for (r in 1:300) {
  cat("Iteration:", r, "\n")
  
  # Create k-fold indices for this repeat
  folds <- createFolds(alzh.forCI$Diagnosis, k = 5)
  #print(folds)
  recall.vect.gbm<-c()
  precision.vect.gbm<-c()
  accuracy.vect.gbm<-c()
  auc.vect.gbm<-c()

  for (i in seq_along(folds)) {
    cat(" Fold:", i, "\n")
    
    # Get index for training and test sets
    #print(folds[[i]])
    test.index <- folds[[i]]
    train.index <- -test.index

    train.fold <- alzh.forCI[train.index, ]
    test.fold <- alzh.forCI[test.index, ]
    
    # Fit the logistic regression model
    model <- gbm(Diagnosis~.,data=train.fold,distribution="bernoulli",n.trees=1000,interaction.depth=4,shrinkage=0.01)
    
    # Get predictions and calculate probabilities on the test set
    probs <- predict(model, test.fold, type = "response")
    preds<-ifelse(probs>0.5,1,0)
    preds <- factor(preds, levels = c("0", "1"))
    actual<-factor(test.fold$Diagnosis,levels=c("0","1"))
    ##confusion matrix
    mtx<-confusionMatrix(preds,actual,positive = "1")

    
    #get recall and precision and accuracy and auc value
    recall<-sum(preds == "1" & actual == "1")/sum(actual == "1")
    #recall2<-mtx$byClass["Sensitivity"]
    recall.vect.gbm<-c(recall.vect.gbm,recall)
    #print(recall.vect)
    #print(recall)
    #print(recall2)
    precision <- mtx$byClass["Pos Pred Value"]
    precision.vect.gbm<-c(precision.vect.gbm,precision)
    
    accuracy<-mtx$overall["Accuracy"]
    accuracy.vect.gbm<-c(accuracy.vect.gbm,accuracy)
    
    roc.curve <- roc(test.fold$Diagnosis, probs)
    auc <- auc(roc.curve)
    auc.vect.gbm<-c(auc.vect.gbm,auc)
    
    
  }
  #print(recall.vect)
  #print(mean(recall.vect))
  recall.set.gbm<-c(recall.set.gbm,mean(recall.vect.gbm))
  precision.set.gbm<-c(precision.set.gbm,mean(precision.vect.gbm))
  accuracy.set.gbm<-c(accuracy.set.gbm,mean(accuracy.vect.gbm))
  auc.set.gbm<-c(auc.set.gbm,mean(auc.vect.gbm))
  
}

```


```{r}
mean.recallgbm.final<-round(mean(recall.set.gbm),4);mean.recallgbm.final
CI.recallgbm.final<-round(t.test(recall.set.gbm)$conf.int,5);CI.recallgbm.final

mean.precisiongbm.final<-round(mean(precision.set.gbm),4);mean.precisiongbm.final
CI.precisiongbm.final<-round(t.test(precision.set.gbm)$conf.int,5);CI.precisiongbm.final

mean.accuracygbm.final<-round(mean(accuracy.set.gbm),4); mean.accuracygbm.final
CI.accuracygbm.final<-round(t.test(accuracy.set.gbm)$conf.int,5);CI.accuracygbm.final

mean.aucgbm.final<-round(mean(auc.set.gbm),4);mean.aucgbm.final
CI.aucgbm.final<-round(t.test(auc.set.gbm)$conf.int,5);CI.aucgbm.final
```

```{r}
Metric.gbm<-c("Recall"=mean.recallgbm.final,"Precision"=mean.precisiongbm.final,"Accuracy"=mean.accuracygbm.final,"AUC"=mean.aucgbm.final)
CI.gbm<-c("Recall CI"="(0.9169,0.9175)","Precision CI"="(0.9464 0.94693)","Accuracy CI"="()","AUC CI"="()")
```



```{r}

Metric.All<-rbind(Metric.regular,Metric.mlr.bestSub,Metric.lasso,Metric.knn,Metric.gbm)
Method<-c("Regular MLR","MLR Best Subset","Lasso","KNN","GBM")
Metric.All<-cbind(Method,Metric.All)
knitr::kable(Metric.All)
```

